```{r, include=FALSE}
#---
#title: "Chapter 4 data analysis"
#author: "Antti Kaihovaara"
#date: "November 25, 2018"
#description: "Chapter 4 data"
#Data : ""MASS"-package. More info from here: https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html"
#---
```

#Exercise 4

##Data 
This time our data includes variables that describe housing values in suburbs of Boston. These are e.g. per capita crime rate, average number of rooms per dwelling and pupil-teacher ratio by town. In total, the data consists of 14 variables and 506 observations.

You can read more about the data from here: https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html


The overview of the variables is shown below. Most attention should be paid to variable "medv"", which describes the median value of the of the owner-occupied homes in thousands of dollars. The values of "medv" range from 5 to 50 with the median 21.20 and mean 22.53. All in all, the variables are in general not normally distributed.


```{r, include=FALSE}
library(ggplot2); library(GGally); library(corrplot); library(tidyr); library(dplyr)

library(MASS)

```

```{r, include=TRUE}
data(Boston)


str(Boston)
dim(Boston)
summary(Boston)
glimpse(Boston)


```
##Graphical overview

The pairs-command below shows us the scatterplot matrices and cor_matrix and corrplot show the correlations between variables. "Medv"" is most correlated with "rm", which means average rooms per dwelling and least correlated with "lstat", which describes percent of the lower status of population.

In the correlation matrix (created by corrplot) the positive correlations are displayed in blue and negative correlationsin red. The color intensity and the square sizes give us visuals on how strong the correlation is. Obviously, we can see the same results as in numerical correlations.

```{r, include=TRUE}
 
pairs(Boston)

cor_matrix<-cor(Boston) %>% round(digits = 2)

cor_matrix 

 
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex = 0.6)

```


##Standardizing the data and creating training and test sets

Next we standardize the dataset to improve comparability between variables that are in different scales. Standardization allows us to look at the standard deviations, which in turn shows how spread the numbers of the variables are. The standardized variables are centered around the mean and have thus both negative and positive values. We scale the variables to satisfy the assumption of normally distributed variables in linear discrimination analysis.

```{r, include=TRUE}

boston_scaled <- scale(Boston)

summary(boston_scaled)


```

Now it is time to use the scaled crime rate to create a categorial variable. We start by changing our scaled object to a data frame and creating a quintile vector of "crim". Then we use the quintiles as break points to create the categorical variable with categories "low", "medium low", "medium high" and "high". Finally, we drop the old crime rate variable from the dataset and add the new categorical value to scaled data.


```{r, include=TRUE}

class(boston_scaled)

boston_scaled <- as.data.frame(boston_scaled)

summary(boston_scaled$crim)

bins <- quantile(boston_scaled$crim)
bins
```

```{r, include=TRUE}
# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

# look at the table of the new factor crime
table(crime)

boston_scaled <- dplyr::select(boston_scaled, -crim)

boston_scaled <- data.frame(boston_scaled, crime)
```

Then it is time to divide the dataset to training and test sets, so that 80% of the data belongs to the training set.

```{r, include=TRUE}
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)
```

##Linear discriminant analysis

Linear discriminant analysis is a classification method that uses dimension reduction technique to recognize patterns in the data. It can be used to find the variables that discriminate/separate the clasess best or predict classes of new data. In this case, our target variable is the categorical "crime" variable we created in the previous section. All the other variables in the training set are used as predictor variables.

We fit the linear discriminant analysis on the training set and and draw the LDA plot.

```{r, include=TRUE}

# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit
```


In the above table we have the target variable "crime" and 13 predictor variables. In the output we have the prior probabilities which are the number of class observations in each class divided  by observations in the whole dataset. Then we have the group means, which are the values of every variable in every class. After that output shows the coefficients of each variable.  Finally, the proportion of trace shows us that the linear discriminant 1 (LD1) explains 95.77 percent of the between group variance. 


```{r, include=TRUE}
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}


classes <- as.numeric(train$crime)

plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1.5)
```


##Prediction with the LDA model

Based on the training model used in the previous section, we classify the observations with LDA. LDA calculates the probababilities for the new observation for belonging in each of the classes. The observation is classified to the class of the highest probability (based on Bayes theorem).

The results are cross tabulated with the crime categories from the test set. The cross tabulation reveals that LDA model is more capable in predicting high crime rates on the test data. For other classes the predictions are correct only roughly little over half of the times. The overall error rate is around 39 %, which is not terrible, but not too good either.


```{r, include=TRUE}


# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
tt <- table(correct = correct_classes, predicted = lda.pred$class)
tt
```

```{r, include=TRUE}
error = sum(tt[row(tt) != col(tt)]) / sum(tt)
error

summary(test)
```


## K-means algorithms on a realoaded dataset

As our final task this time, we test what is the optimal number of clusters in the dataset. We do this with k-means algorithm, but first we must reload the data,  standardize the dataset and scale the variables to comparable distances.


```{r, include=TRUE}
library(MASS)
data('Boston')

# center and standardize variables
boston_K_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_K_scaled)

# class of the boston_scaled object
class(boston_K_scaled)

# change the object to data frame
boston_K_scaled <- as.data.frame(boston_K_scaled)

```


We calculate the distances between the observations with dist-function and Manhattan method.


```{r, include=TRUE}
# euclidean distance matrix
dist_eu <- dist(boston_K_scaled)

# look at the summary of the distances
summary(dist_eu)

``` 

```{r, include=TRUE}
# manhattan distance matrix
dist_man <- dist(boston_K_scaled, method = 'manhattan')

# look at the summary of the distances
summary(dist_man)
``` 

K-means is an unsupervised clustering method that assigns observations based on their characteristics. In this exercise we use 3 centers. K-means might produce different results every time, because it randomly assigns the initial cluster centers. To deal with that, we use the set.seed -function. 

In this case the right number of clusters seems to be 2. We therefore run kmmeans againd with 2 clusters.



```{r, include=TRUE}
# k-means clustering
km <-kmeans(boston_K_scaled, centers = 3)

# plot the Boston dataset with clusters
pairs(boston_K_scaled, col = km$cluster)
```

```{r, include=TRUE}
set.seed(123) #K-means might produce different results every time, because it randomly assigns the initial cluster centers. This function is used to deal with that.

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_K_scaled, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

```

```{r, include=TRUE}
# k-means clustering
km <-kmeans(boston_K_scaled, centers = 2, nstart = 20)
km


# plot the Boston dataset with clusters
pairs(boston_K_scaled[1:5], col = km$cluster)


pairs(boston_K_scaled[6:10], col = km$cluster)

```

The clustering exercise was not a huge success as the model explains only about 35 % variance in the dataset. This implies a poor fit.


